{"cells":[{"cell_type":"markdown","metadata":{"id":"gyBmm0gi8yqR"},"source":["# Pandas: Working with DataFrames\n","\n","## Table of Contents\n","\n","1. [**Introduction**](#Intro)\n","2. [**DataFrame**](#DataFrm)\n","3. [**Selection and Slicing**](#SelSlic)\n","4. [**Sorting**](#Sort)\n","5. [**Summarizing and Descriptive Statistics**](#Summary)\n","6. [**Importing Data Sets**](#import)\n","\n","    6.1. [**CSV Files**](#csv)\n","\n","    6.2. [**Excel Files**](#excel)\n","\n","    6.3. [**GeoJASON Files**](#jason)\n","\n","7. [**Miscellaneous**](#misc)\n"]},{"cell_type":"markdown","metadata":{"id":"b2SR5BLm9Uu2"},"source":["## 1. Introduction <a name=\"Intro\"></a>\n","\n","__pandas__ will be one of the major tool that we use for much of the rest of the semester. It contains data structures and data manipulation tools designed to make data cleaning and analysis fast and easy in Python.\n","\n","pandas is often used in tandem with numerical computing tools like _NumPy_ and _SciPy_, analytical libraries like _statsmodels_ and _scikit-learn_, and data visualization libraries like _matplotlib_. Pandas adopts significant\n","parts of NumPyâ€™s style of array-based computing.\n","\n","However, _pandas_ is designed for working with tabular or heterogeneous data. _NumPy_, by contrast, is best suited for working with homogeneous numerical array data.\n","\n","Let's import the library and start learning how to use it!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"keYnOYfc8z8i"},"outputs":[],"source":["import pandas as pd     # importing the library"]},{"cell_type":"markdown","metadata":{"id":"LL_WBZf3A_V2"},"source":["## 2. DataFrame <a name=\"DataFrm\"></a>\n","\n","A DataFrame represents a rectangular table of data and contains an ordered collection of columns, each of which can be a different value type (numeric, string, boolean, etc.). The DataFrame has both a row and column index. Each column of a dataframe is a _Series_. We will talk about some of the properties of the _Series_ later if needed. If you want to know more about Series, you can take a look at this link: https://towardsdatascience.com/pandas-series-a-lightweight-intro-b7963a0d62a2\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1Jl2U_DrjXiNqBXZKFD2oOC-b9TPUv7Pv)\n","\n","But for now, let's focus on DataFrames. There are multiple ways to construct a DataFrame, though one of the most common one is from a dictionary of equal-length lists or _NumPy_ arrays:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gh6qI1YN_PSa"},"outputs":[],"source":["data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n","        'year': [2000, 2001, 2002, 2001, 2002, 2003],\n","        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n","\n","frame = pd.DataFrame(data)\n","frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pp6o60WNBb6g"},"outputs":[],"source":["frame.head()     # if you have a large dataframe and want to see only the first 5 rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYn7-K73Y7_D"},"outputs":[],"source":["frame.tail(2)    # if you have a large dataframe and want to see only the last 3 rows"]},{"cell_type":"markdown","metadata":{"id":"fDCufIduB-Xg"},"source":["You can re-arrange the order with which the columns are represented while forming the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8Uw_HEwBxKr"},"outputs":[],"source":["frame2=pd.DataFrame(data, columns=['year', 'state', 'pop'])\n","frame2"]},{"cell_type":"markdown","metadata":{"id":"yqDgsYFnekzj"},"source":["We can __add a new column__ by just assigning a name and the values for that new column as long as that column did not have exist from before in our dataframe:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4w2tIGreq3U"},"outputs":[],"source":["frame2['area'] = [44825,44825,44825,110567,110567,110567]\n","frame2"]},{"cell_type":"markdown","metadata":{"id":"SrKQ_HSZivzq"},"source":["We can also __drop a column__ using the <font color='blue'>drop</font> method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWwkAclqjAt9"},"outputs":[],"source":["# Defining the dataframe again\n","data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n","        'year': [2000, 2001, 2002, 2001, 2002, 2003],\n","        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n","frame = pd.DataFrame(data)\n","frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWtveu8yiXGA"},"outputs":[],"source":["frame.drop(columns='state', inplace=True)   \n","frame "]},{"cell_type":"markdown","metadata":{"id":"YvOBM6N4SHlE"},"source":["You can see a list of columns' names:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o86cFxWgCLAs"},"outputs":[],"source":["frame.columns"]},{"cell_type":"markdown","metadata":{"id":"0qJ0OMO1fbTz"},"source":["We can also check if a specific column exists in the columns of a dataframs using <font color='blue'>in</font>. The output is <font color='green'>True</font> if it does and <font color='green'>False</font> if it does not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pD2ZpWwqfv5-"},"outputs":[],"source":["'year' in frame.columns     # is 'year' a column in dataframe frame ?"]},{"cell_type":"markdown","metadata":{"id":"ZyBO9gbbgmPc"},"source":["<font color='orange'> __Note__:</font> The _index_ values are autoamtically created and start from __0__ when you create a DataFrame. However, you can explicitly define the index values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh266GNug8vM"},"outputs":[],"source":["data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n","        'year': [2000, 2001, 2002, 2001, 2002, 2003],\n","        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFpWb8NujPPZ"},"outputs":[],"source":["frame3 = pd.DataFrame(data,columns=['year','state','pop'],\n","                      index=['row 1','row 2','row 3', 'row 4','row 5','row 6'])\n","print(frame3)         # Print the DataFrame\n","print('')\n","print(frame3.index)   # See a list of index values"]},{"cell_type":"markdown","metadata":{"id":"FUPSyg5PTOGX"},"source":["<font color='red'>__Question (1)__</font>: Define a dataframe, named _dfq_ using the following dictionary and then print the output for each section.\n","```python\n","dictq=\n","{'Rank':[1,2,3,4,5],\n","'Country':['United States','China','Japan','Germany','India'],\n","'GDP (US $Million)':[21427700,14342903,5081770,3845630,2875142]}\n","```\n","- Show the first three rows of the dataframe\n","- Add the following as a new column to the dataframe:\n","```python\n","'Area sq mi':[3796742, 3705407, 145937, 137847, 1269219]\n","```\n","- Drop the _Rank_ column\n","- Check if label _area_ is one of the column labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RRxjX6MS283"},"outputs":[],"source":["## In-class Assignment\n"]},{"cell_type":"markdown","metadata":{"id":"gJFc4eFhTyjF"},"source":["## 3. Selection and Slicing <a name=\"SelSlic\"></a>\n","Selecting speific columns and rows is quite similar to the way we make the slections in _Numpy_. For example, to select specific __columns__:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqC4TbkUUNfV"},"outputs":[],"source":["frame2=pd.DataFrame(data, columns=['year', 'state', 'pop'],\n","                    index=[\"R1\", \"R2\", \"R3\", \"R4\", \"R5\", \"R6\"])\n","frame2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvKB6lM9xI_r"},"outputs":[],"source":["frame2['state']                   # This produces a panda series. Check the data type using the command type(frame2['state'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPZkLCApVAjC"},"outputs":[],"source":["frame2[['state']]                 # This produces a dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VG3688c5U1nT"},"outputs":[],"source":["frameNew=frame2[['year','state']]  # Selecting two columns and putting them in a new dataframe\n","frameNew.head(3)                   # printing only the first three rows"]},{"cell_type":"markdown","metadata":{"id":"9PC9kGeOagyH"},"source":["You can select specific __rows__ in the same way you do for dictionaries and arrays in _Numpy_."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28PmDafVV-nJ"},"outputs":[],"source":["print(frame2[0:2])                              # Selecting the first two rows\n","print('')\n","print(frame2[:2])                               # Selecting the first 2 rows "]},{"cell_type":"markdown","metadata":{"id":"y44WtZwobSa3"},"source":["In order to make specific slices of a dataframe in pandas, we can use <font color='blue'>loc</font> and <font color='blue'>iloc</font> methods. They enable you to select a subset of the rows and columns from a DataFrame with NumPy-like notation using either axis labels (loc) or integers (iloc).\n","\n","Let's define a new dataframe and use it to take a look at the way loc and iloc methods work. This time, I am using a different method to create a DataFrame. I define a 3 by 3 matrix (a 2D array)as the base and then define index and column names."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaxcurvZaMsw"},"outputs":[],"source":["import numpy as np\n","df = pd.DataFrame( np.arange(9).reshape((3, 3)),\n","                  index=['a', 'c', 'd'],\n","                  columns=['Ohio', 'Texas', 'California'])\n","df"]},{"cell_type":"markdown","metadata":{"id":"qrCA_eNPlHSz"},"source":["When we use the method <font color='blue'>loc</font>, we make selections based on the index and column labels:\n","![alt text](https://docs.google.com/uc?export=download&id=1isFZwrMJRCvvI8se3Zxk9PZzkiygOkmg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eci9pvg2wsFj"},"outputs":[],"source":["df.loc[['a']]                            # select a specific row, outputs a frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McXKkU7pxyi8"},"outputs":[],"source":["df.loc['a']                              # select a specific row, outputs a series"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06BN6AsJx67I"},"outputs":[],"source":["df.loc['c','Ohio']                       # selecting row c, column Ohio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02GczPJcayDu"},"outputs":[],"source":["df.loc[['a','c'],['Ohio','California']]  # selecting rows a & c, columns Ohio & California"]},{"cell_type":"markdown","metadata":{"id":"LDIgy6r8lXu6"},"source":["When we use the method <font color='blue'>iloc</font>, we make the selections based on the index and column integer values:\n","![alt text](https://docs.google.com/uc?export=download&id=1folfwTBlCldBpDrPGNntuZ2uFaojFkTz)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8ys1KMxoGRI"},"outputs":[],"source":["df.iloc[[0]]                    # select a specific row, outputs a dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7MysvqloR1y"},"outputs":[],"source":["df.iloc[[1],[0]]                # selecting row c, column Ohio, outputs a dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cgYyJLcyYCg"},"outputs":[],"source":["df.iloc[[0,1],[0,2]]            # selecting rows a & c, columns Ohio & California"]},{"cell_type":"markdown","metadata":{"id":"WYqzrx1sIguy"},"source":["Some other examples of using `loc` and `iloc` for slicing "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUp7pT2fHubA"},"outputs":[],"source":["df.loc[:,:'Texas']                   # All the rows with columns up to Texas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gkA0PnZlfWx"},"outputs":[],"source":["df.loc['c':,:'Ohio']                  # Row c and d with the first column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTMZgX6TG3ZJ"},"outputs":[],"source":["df.iloc[:,:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGmXG5TzIN7_"},"outputs":[],"source":["df.iloc[:,:2]                   # All the rows and columns up to Texas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16MReYpMIQmB"},"outputs":[],"source":["df.iloc[1:,:1]                   # Row c and d and the first column, Ohio"]},{"cell_type":"markdown","metadata":{"id":"RV8AkjNy13IR"},"source":["Following table summarizes the slicing and selection possibilities for Series type outputs.\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1aqTJlhANKLOVsYtZdnp0Gsx8M-1Bxcd-)\n","![alt text](https://docs.google.com/uc?export=download&id=1I1YNHbzdfylM_bugaf9AXacdggm-NyJM)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNE7xYDzogwn"},"outputs":[],"source":["df.iat[0,1]"]},{"cell_type":"markdown","metadata":{"id":"AkRtVS-j1r-W"},"source":["<font color='red'>__Question (2)__</font>: Define a dataframe, called ```df_q``` with four columns of ```Car```, ```Model Year```, ```Propulsion```, and ```Time``` for the first five cars (https://en.wikipedia.org/wiki/List_of_fastest_production_cars_by_acceleration). Print the output for each section.\n","- Define a new _DataFrame_ with only 'Car' and 'Time' columns.\n","- Select the first three rows of the original dataframe, df_q\n","- Make a slice that includes only 'Car', 'Propulsion' and 'Time' columns for the first two cars."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWPmqzrP1Z5v"},"outputs":[],"source":["## In-Class Assignment\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DCE0akZr38kl"},"source":["## 4. Sorting <a name=\"Sort\"></a>\n","\n","Sorting a dataset by some criterion is another important operation when performing data analysis. To sort by row index or column label, use the <font color='blue'>sort_index</font> method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trPr5RfA3_4x"},"outputs":[],"source":["import numpy as np       # just to make sure numpy is imported!\n","\n","frame = pd.DataFrame(np.arange(12).reshape((3, 4)),\n","                     index=[2,1,4],\n","                     columns=['d', 'a', 'b', 'c'])\n","frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tlWPSpM4YoY"},"outputs":[],"source":["frame.sort_index()             # Sorting along index axis 0, default"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7u4982nE7hxJ"},"outputs":[],"source":["frame.sort_index(axis=1)       # Sorting along index axis 1, column labels"]},{"cell_type":"markdown","metadata":{"id":"iYkteUwt1R6N"},"source":["<font color='orange'> __Note__:</font> Sorting does not change the frame by default. You can change that by setting ```inplace=True```."]},{"cell_type":"markdown","metadata":{"id":"rDjp6nh07uCi"},"source":["you can use the data in one or more columns as the sort keys. To do so, pass one or more column names to the <font color='blue'>by</font> option of <font color='blue'>sort_values</font> method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkAvdDcw8Cgd"},"outputs":[],"source":["frame = pd.DataFrame({'b': [4, 7, -3, 2], 'a': [2, 1, 0, 1]})\n","frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTrzFE0R8Eoj"},"outputs":[],"source":["frame.sort_values(by='b')      # sort the dataframe based on the values on column b"]},{"cell_type":"markdown","metadata":{"id":"z0ZQsGql8RPn"},"source":["To sort using multiple columns, pass a list of names:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxWNqMoF8T5z"},"outputs":[],"source":["frame.sort_values(by=['a', 'b'])"]},{"cell_type":"markdown","metadata":{"id":"ztg6vbnk85en"},"source":["<font color='red'>__Question (3)__</font>: Use the dataframe you defined for the previous question, i.e. the list of fastest cars. Print the output for each section of the question.\n","\n","- Sort the cars based on their Time, fastes at the top\n","- Sort the cars based on theirTime, slowest at the top\n","- Sort the cars based on the two columns of Model Year and Time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQ-qhRNE86r3"},"outputs":[],"source":["## In-Class Assignment\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6TGNHUz45Pt5"},"source":["## 5. Summarizing and Descriptive Statistics <a name=\"Summary\"></a>\n","\n","**Pandas** objects are equipped with a set of common mathematical and statistical methods. Most of these fall into the category of reductions or summary statistics, methods that extract a single value (like the sum or mean) from the rows or columns of a DataFrame.\n","\n","Let's create a DataFrame with some missing values, mimicking a common situation when dealing with real world data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlUN3iyI5WL-"},"outputs":[],"source":["# make sure numpy is imported as np\n","\n","df=pd.DataFrame([[1.4, np.nan], [7.1, -4.5],\n","              [np.nan, np.nan], [0.75, -1.3]],\n","             index=['a', 'b', 'c', 'd'],\n","             columns=['one', 'two'])\n","df"]},{"cell_type":"markdown","metadata":{"id":"qFVmqqw-OhuZ"},"source":["As you can see, we have three missing values in our DataFrame. The reason we choose this type of data frame is to show that Pandas function for summarizing and descriptive statistics have built-in handling for missing data.\n","\n","Let's take a look at some of these methods:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EX-XhadPMLD"},"outputs":[],"source":["print(df)                       # Printing the data frame for reference\n","print('------------------------------------')\n","print('Sum of the values down the rows (along axis 0):')\n","print(df.sum())                 # returns a Series containing column sums\n","print('')\n","# Passing axis='columns' or axis=1 sums across the columns instead\n","print('Sum of the values across the columns (along axis 1):')\n","print(df.sum(axis='columns'))"]},{"cell_type":"markdown","metadata":{"id":"1SfbxJG6P39K"},"source":["NA values are excluded unless the entire slice (row or column in this case) is NA. This can be disabled with the <font color='blue'>skipna</font> option."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gn16DkVLPUSi"},"outputs":[],"source":["df.mean(axis='columns')     # Average of the values across the columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rngu-nMvQhaR"},"outputs":[],"source":["df.cumsum()                               # Cumulative some of values"]},{"cell_type":"markdown","metadata":{"id":"YHIyl5YlQ7YU"},"source":["A very common method that we use when doing exploratory data analysis is <font color='blue'>describe</font>, which produces multiple summary statistics in one shot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pPFfKvGRCPG"},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{"id":"B2T-b1ZM_Ile"},"source":["By default, method `describe` returns the statistics for numerical values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVd1_eP18u3e"},"outputs":[],"source":["dff = pd.DataFrame({\"Numeric\":[1,2,3],\n","                    \"Categorical\":pd.Categorical(['cat1', 'cat2', 'cat']),\n","                    \"object\":['obj1', 'obj2', 'obj2']})\n","dff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7iz2D--M8_ip"},"outputs":[],"source":["dff.describe()"]},{"cell_type":"markdown","metadata":{"id":"ZhHllip7RVPM"},"source":["The table below lists all the available methods for descriotive and summary statistics. To know more about each function, just follow the general approach to look at the corresponding help page. For instance to know more about <font color='blue'>count</font> type:\n","```python\n","df.count?\n","```\n","\n","![alt text](https://docs.google.com/uc?export=download&id=199dnEnf0YzGUbXMgyxhne3O1LWQbjpmJ)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FZ8m50xoSNjK"},"source":["<font color='red'>__Question (4)__</font>: For the DataFrame of the fastest cars, print the output for each section:\n","\n","- Find the mean value of column _Time_.\n","- Get the summary descriptive for the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAklMAqdROlu"},"outputs":[],"source":["## In-Class Assignment\n"]},{"cell_type":"markdown","metadata":{"id":"MHsBr9j89_Vl"},"source":["## 6. Importing Datasets <a name=\"import\"></a>\n","\n","Accessing data is a necessary first step for data analysis. We always use __Pandas__ for importing and exporting data thorough this course. In programming language, they refere to reading data from a file as _parsing_.\n","\n","When you do data analysis, you mainly have to import/ export/ deal with one of the following type of data:\n","\n","- Text files and other more efficient on-disk formats scuh as CSV files, Excel files, HTML files, GeoJSON files, etc\n","- Loading data from databases\n","- Interacting with network sources like web APIs\n","\n","We will mainly focus on CSV and Excel files since they are the most common type of data types for small datasets. Following table gives you a list of available parsing functions in _pandas_ that you can use to import data. The most important ones that we will use very frequently are <font color='blue'>read_csv</font> and <font color='blue'>read_excel</font>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1iCFVQBV5EjlthrZjqmCUQPm0SHuB56z7)\n","![alt text](https://docs.google.com/uc?export=download&id=1KBc_73mkQkRZiDdkcLFY7o_brhUP6aOi)"]},{"cell_type":"markdown","metadata":{"id":"2DtiJbkkVDwM"},"source":["Let's see how can we use these to import some data sets.\n","\n","\n","### 6.1. CSV Files <a name=\"csv\"></a>\n","\n","The first data set is the data set for the experiments done by NASA on a series of airfoils to measure the noise generated due to the fluid-solid interaction in a wind tunnel. This data set is a CSV file and it is located at https://raw.githubusercontent.com/MasoudMiM/ME_364/main/Airfoil_noise/Airfoil_Noise.csv. The file has no header to describe the values in each column but the description of the data is given here: https://raw.githubusercontent.com/MasoudMiM/ME_364/main/Airfoil_noise/READ_ME.\n","\n","Therefore, we know that the first five columns are:\n","1. Frequency, in Hertzs. \n","2. Angle of attack, in degrees. \n","3. Chord length, in meters. \n","4. Free-stream velocity, in meters per second. \n","5. Suction side displacement thickness, in meters. \n","\n","and the last column is:\n","6. Scaled sound pressure level, in decibels. \n","\n","So we use options <font color='green'>header</font> for <font color='blue'>read_csv</font> command to mention that the file has no header and <font color='green'>names</font> to provide names fo the columns for the data set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIBh9p-M-E-U"},"outputs":[],"source":["url = 'https://raw.githubusercontent.com/MasoudMiM/ME_364/main/Airfoil_noise/Airfoil_Noise.csv'   # Link to the Airfoil Noise data set\n","df1 = pd.read_csv(url,header=None, names=['Frequency (Hz)','Attack_Angle (deg)','Cord (m)','FS_Velocity (m/s)','SSD_Thickness (m)','Sound_Pressure_Level (dB)'])\n","\n","# Dataset is now stored in a Pandas's Dataframe\n","df1.head(10)"]},{"cell_type":"markdown","metadata":{"id":"40pVs8CW3Qa_"},"source":["If the file is on your computer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjWREujN3dcf"},"outputs":[],"source":["import io\n","from google.colab import files\n","\n","uploadfile = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUAfsUms36eB"},"outputs":[],"source":["dfnew = pd.read_csv(io.BytesIO(uploadfile['Data.csv']),\n","                    header=None,names=['Frequency (Hz)','Attack_Angle (deg)','Cord (m)','FS_Velocity (m/s)','SSD_Thickness (m)','Sound_Pressure_Level (dB)'])\n","dfnew.head()"]},{"cell_type":"markdown","metadata":{"id":"p2o6UQjWXwDq"},"source":["\n","### 6.2. Excel Files <a name=\"excel\"></a>\n","\n","The second data set is fuel economy data which are the result of vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA. The data file is an Excel file and it is located at https://raw.githubusercontent.com/MasoudMiM/ME_364/main/EPA_Green_Vehicle_Guide/Data.xlsx. You can also find a very good description of the data here: https://github.com/MasoudMiM/ME_364/blob/main/EPA_Green_Vehicle_Guide/Data_Description.pdf\n","\n","\n","The is an Excel file so we need to first give the address to the file using <font color='blue'>ExcelFile</font> and then import the specific sheet from the file using the command <font color='blue'>read_excel</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVIazuHqXOih"},"outputs":[],"source":["xlsx = pd.ExcelFile('https://raw.githubusercontent.com/MasoudMiM/ME_364/main/EPA_Green_Vehicle_Guide/Data.xlsx')\n","df2 = pd.read_excel(xlsx, 'Sheet1')            # first sheet of the excel file\n","\n","# Dataset is now stored in a Pandas's Dataframe\n","df2"]},{"cell_type":"markdown","metadata":{"id":"bl9ppJhSghys"},"source":["### 6.3. JSON Files <a name=\"jason\"></a>\n","\n","JSON (short for JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more free-form data format than a tabular text form like CSV. Here, is an example:\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1ZXeE9eA9EINEHFiEY6Y3i4rcWFiY1dWQ)\n","\n","\n","How you convert a JSON object or list of objects to a DataFrame or some other data structure for analysis will be up to you. Conveniently, you can pass a list of dicts (which were previously JSON objects) to the DataFrame constructor and select a subset of the data fields. A very comon application of JSON files is when we are dealing with geographic data structure."]},{"cell_type":"markdown","metadata":{"id":"mafnI2Y2cJtq"},"source":["## 7. Miscellaneous <a name=\"misc\"></a>"]},{"cell_type":"markdown","metadata":{"id":"EWm-LwozcWnz"},"source":["### <font color='green'>Unique Values</font>\n","\n","If you want to find the unique values in a specific column or part of a column in a dataframe, you can use the module `unique` in pandas. For instance, let's find how many unique value of frequencies do we have in the column Frequency for Airfoil Noise data set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6b1imTxUUoV"},"outputs":[],"source":["url = 'https://raw.githubusercontent.com/MasoudMiM/ME_364/main/Airfoil_noise/Airfoil_Noise.csv'   # Link to the Airfoil Noise data set\n","df1 = pd.read_csv(url, header=None, names=['Frequency (Hz)','Attack_Angle (deg)','Cord (m)','FS_Velocity (m/s)','SSD_Thickness (m)','Sound_Pressure_Level (dB)'])\n","\n","# Dataset is now stored in a Pandas's Dataframe\n","df1.head(6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFriA_rgUd7q"},"outputs":[],"source":["df1['Frequency (Hz)'].unique()"]},{"cell_type":"markdown","metadata":{"id":"qZjA1B1rU2Z5"},"source":["So while there are total number of 1503 rows of measurements, we only have 21 unique values for frequencies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrE8IcHJUjSo"},"outputs":[],"source":["print('Total number of rows:',len(df1['Frequency (Hz)']))\n","print('Number of unique values: ',len(df1['Frequency (Hz)'].unique()))"]},{"cell_type":"markdown","metadata":{"id":"rC3afsNfeJez"},"source":["### <font color='green'>Where method</font>\n","\n","This module is mainly used to replce specific values in a dataframe where a given condition is False."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nDAfRE6ZJrV"},"outputs":[],"source":["dftest=pd.DataFrame({'clm1':[1,2,3,4,5],'clm2':[4,5,6,7,8],'clm3':[9,7,5,1,0]})\n","dftest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAkudOzM6T9s"},"outputs":[],"source":["dftest[ dftest['clm1']<3 ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8Bv_ul8ZbkE"},"outputs":[],"source":["dftest['clm1'].where(dftest['clm1']<3,0,inplace=True)   # Where cond is True, keep the original value. Where False, replace with corresponding value \n","dftest"]},{"cell_type":"markdown","metadata":{"id":"GP5dWx8pebBs"},"source":["### <font color='green'>Select rows & Masking</font>\n","\n","There are so many situations in which you need to select specific rows whose column value equals to a specific number or a string."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wq-3NdsVlaHG"},"outputs":[],"source":["dftest=pd.DataFrame({'clm1':[1,2,2,4,5],'clm2':[4,5,5,5,8],'clm3':[9,9,9,1,0]})\n","dftest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYPJKf1hlqlK"},"outputs":[],"source":["# To select rows of the dataframe with values in column1=2\n","dftest[ dftest['clm1']==2 ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSIs4d-OnPdH"},"outputs":[],"source":["# To select rows of the dataframe with values in column2=5\n","dftest[ dftest['clm2']==5 ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkmSq-ZSn6mX"},"outputs":[],"source":["# To select rows of the dataframe with values in column3 not equal to 1\n","dftest[ dftest['clm3']!=1 ] "]},{"cell_type":"markdown","metadata":{"id":"ScOp7R4baIgG"},"source":["<font color='red'>__Question (5)__</font>: Use the airfoil noise dataset to asnwer the following:\n","- Find the number of unique values of sound pressure levels\n","- Define a new dataframe using all the rows with values of Frequency equal to 1000 Hz."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLf5FgLHabny"},"outputs":[],"source":["# In-Class Assignment\n"]},{"cell_type":"markdown","metadata":{"id":"mrnvWaenHwol"},"source":["### <font color='green'>Combining DataFrames</font>\n","\n","With **Pandas**, you can merge, join, and concatenate your datasets, allowing you to unify and better understand your data as you analyze it. Here is a great read on how to do it and what are the differences between these three: https://realpython.com/pandas-merge-join-and-concat/"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"3_Pandas.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
